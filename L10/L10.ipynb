{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a07f6d73-8bbb-4832-b0cd-a3f30aba009d",
      "metadata": {
        "id": "a07f6d73-8bbb-4832-b0cd-a3f30aba009d"
      },
      "source": [
        "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/C3669C-2025-01/blob/main/L10/L10.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae6ff1dc-2734-497d-be36-279f2dc76beb",
      "metadata": {
        "id": "ae6ff1dc-2734-497d-be36-279f2dc76beb"
      },
      "source": [
        "# Setup and Installation\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\" width=\"20%\" height=\"auto\"/>\n",
        "\n",
        "You **MUST** run this Jupyter notebook at Google Colab.  We will using **unsloth** library. **Unsloth** makes finetuning large language models like Llama-3 2X faster and using 70% less memory with no degradation in accuracy.\n",
        "\n",
        "The following Python code was modified/adapted from Unsloth.\n",
        "\n",
        "### References\n",
        "- [github/unsloth](https://github.com/unslothai/unsloth)\n",
        "- [Unsloth documentation](https://docs.unsloth.ai/)\n",
        "- [huggingface/unsloth](https://huggingface.co/unsloth)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35db9108-5cd7-4d05-ab39-6ca01fa370d2",
      "metadata": {
        "id": "35db9108-5cd7-4d05-ab39-6ca01fa370d2"
      },
      "source": [
        "### Google Colab\n",
        "- At Google Colab, Go to `Runtime` > `Change runtime type` and select `T4 GPU`. Or if you are lucky, choose `A100 GPU`.\n",
        "- Using T4 GPU is free with limited access time\n",
        "- If you have Colab+ subscription, you can select A100 GPU.\n",
        "\n",
        "<img src=\"https://github.com/koayst-rplesson/C3669C-2025-01/blob/main/L10/colab-01.png?raw=1\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f63cfe5f-e947-458c-82f6-d6a2ca4fda98",
      "metadata": {
        "id": "f63cfe5f-e947-458c-82f6-d6a2ca4fda98"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install unsloth\n",
        "\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e23847-01ec-41ad-9879-153f503cc11a",
      "metadata": {
        "id": "16e23847-01ec-41ad-9879-153f503cc11a"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None          # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True   # Use 4bit quantization to reduce memory usage. Can be False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e271b221-9742-42ed-9192-69db51006784",
      "metadata": {
        "id": "e271b221-9742-42ed-9192-69db51006784"
      },
      "outputs": [],
      "source": [
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5405f52d-95a7-4ba2-8d9f-41d7adbb5b5d",
      "metadata": {
        "id": "5405f52d-95a7-4ba2-8d9f-41d7adbb5b5d"
      },
      "outputs": [],
      "source": [
        "# if you happened to use one of the gated models (for example: meta/llama 3.2), you need to go to huggingface to apply for permission to access.\n",
        "# Possible error message looks like this:\n",
        "#    Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list.\n",
        "#    Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.`\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e1e3fe1-3c9b-4d4d-b49f-4007107aaf53",
      "metadata": {
        "id": "8e1e3fe1-3c9b-4d4d-b49f-4007107aaf53"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02bc4fb5-d612-4851-962c-fb95261f4c19",
      "metadata": {
        "id": "02bc4fb5-d612-4851-962c-fb95261f4c19"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "\n",
        "    target_modules = [\"q_proj\",\n",
        "                      \"k_proj\",\n",
        "                      \"v_proj\",\n",
        "                      \"o_proj\",\n",
        "                      \"gate_proj\",\n",
        "                      \"up_proj\",\n",
        "                      \"down_proj\",],\n",
        "\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcb6982f-4090-4995-a95e-cdcc47c8ce36",
      "metadata": {
        "id": "dcb6982f-4090-4995-a95e-cdcc47c8ce36"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Preparation\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e822a697-6779-4ed3-938c-98174cf120ac",
      "metadata": {
        "id": "e822a697-6779-4ed3-938c-98174cf120ac"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "903ac708-f84e-4595-8b65-c2c5baf3eef0",
      "metadata": {
        "id": "903ac708-f84e-4595-8b65-c2c5baf3eef0"
      },
      "outputs": [],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3325f3d-844d-4d8b-ae43-069b59f5d904",
      "metadata": {
        "id": "d3325f3d-844d-4d8b-ae43-069b59f5d904"
      },
      "outputs": [],
      "source": [
        "print(f\"Output:\\n{dataset[0]['output']}\\n\")\n",
        "print(f\"Input:\\n{dataset[0]['input']}\\n\")\n",
        "print(f\"Instruction:\\n{dataset[0]['instruction']}\\n\")\n",
        "print(f\"Text:\\n{dataset[0]['text']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a15d7ee-cea0-4272-b17d-d834126277ef",
      "metadata": {
        "id": "7a15d7ee-cea0-4272-b17d-d834126277ef"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa217556-7984-48cb-b122-5208878bb6ca",
      "metadata": {
        "id": "aa217556-7984-48cb-b122-5208878bb6ca"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c3f0847-7d17-46b3-b96b-f454b6c1f1a0",
      "metadata": {
        "id": "3c3f0847-7d17-46b3-b96b-f454b6c1f1a0"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebbb4f94-8023-452e-98b5-2098cc51bc36",
      "metadata": {
        "id": "ebbb4f94-8023-452e-98b5-2098cc51bc36"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0ee1f4a-98d6-493e-a437-bc0dcffaa18c",
      "metadata": {
        "id": "c0ee1f4a-98d6-493e-a437-bc0dcffaa18c"
      },
      "outputs": [],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6118073-c1e6-465a-a116-cc39b92f4e3c",
      "metadata": {
        "id": "c6118073-c1e6-465a-a116-cc39b92f4e3c"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b4c5d5-5ee0-45bb-9865-468a8b4dd8c8",
      "metadata": {
        "id": "27b4c5d5-5ee0-45bb-9865-468a8b4dd8c8"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "451dc78c-ed93-4979-9b4c-9bfce9ee6610",
      "metadata": {
        "id": "451dc78c-ed93-4979-9b4c-9bfce9ee6610"
      },
      "source": [
        "You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "911e502e-1758-46ee-9422-8a0fffdc2ce2",
      "metadata": {
        "id": "911e502e-1758-46ee-9422-8a0fffdc2ce2"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output/response - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4193f44d-98da-4825-b263-e260d9519e83",
      "metadata": {
        "id": "4193f44d-98da-4825-b263-e260d9519e83"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce2bc6f-c82d-4847-b2b9-179987495256",
      "metadata": {
        "id": "0ce2bc6f-c82d-4847-b2b9-179987495256"
      },
      "outputs": [],
      "source": [
        "# connect to google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f456cc49-9380-4dd8-b650-a3ba2d1773c6",
      "metadata": {
        "id": "f456cc49-9380-4dd8-b650-a3ba2d1773c6"
      },
      "outputs": [],
      "source": [
        "path_to_saved_model = \"/content/drive/MyDrive/Colab Notebooks/c3669c/L06/lora_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71435773-e19e-45b3-bd4e-8fee9a94359f",
      "metadata": {
        "id": "71435773-e19e-45b3-bd4e-8fee9a94359f"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(path_to_saved_model) # Local saving\n",
        "tokenizer.save_pretrained(path_to_saved_model)\n",
        "\n",
        "# you can push to huggingface if you have an account\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe03aee-35d3-4249-9aca-38a2ada511da",
      "metadata": {
        "id": "9fe03aee-35d3-4249-9aca-38a2ada511da"
      },
      "outputs": [],
      "source": [
        "# check the files are save to directory lora_model\n",
        "\n",
        "!ls -al \"/content/drive/MyDrive/Colab Notebooks/c3669c/L06/lora_model\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8afc5ee-1c94-4e01-84f1-63a932e8fd9d",
      "metadata": {
        "id": "c8afc5ee-1c94-4e01-84f1-63a932e8fd9d"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2c67e5-d0cc-407d-8ece-4fcc50fbe617",
      "metadata": {
        "id": "ae2c67e5-d0cc-407d-8ece-4fcc50fbe617"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = path_to_saved_model, # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f6b1c49-0668-4787-89e7-ec7396aead14",
      "metadata": {
        "id": "6f6b1c49-0668-4787-89e7-ec7396aead14"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = You MUST copy from above!\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4179207-85ab-41ba-8029-0b11ed3252ce",
      "metadata": {
        "id": "c4179207-85ab-41ba-8029-0b11ed3252ce"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"What is a famous tall tower in Paris?\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "808bec63-ca41-409a-88a5-e49ac0740336",
      "metadata": {
        "id": "808bec63-ca41-409a-88a5-e49ac0740336"
      },
      "source": [
        "### Cleanup\n",
        "\n",
        "If you don't need the models anymore, remember to go to `path_to_saved_model` and delete the directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "718b7068-8d86-4383-a28a-65e982927554",
      "metadata": {
        "id": "718b7068-8d86-4383-a28a-65e982927554"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}