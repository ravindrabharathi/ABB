{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeDlZxLX9B3H"
      },
      "source": [
        "# Automated GenAI Newsletter Generator\n",
        "\n",
        "# Author: Ravindra Bharathi\n",
        "# Email: ravindra.graicells@gmail.com\n",
        "# Date: 2025-02-27\n",
        "\n",
        "## Setup Environment\n",
        "### Install dependencies:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R7mhFQjS9B3I"
      },
      "outputs": [],
      "source": [
        "!pip install arxiv python-dotenv PyGithub python-pptx pandas --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sLvVwG5HU7_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai --upgrade --quiet"
      ],
      "metadata": {
        "id": "_xCmz8gl9lki"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mKJvJQl9B3J"
      },
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xtt9wJ4u9B3K"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "import ssl\n",
        "import re\n",
        "\n",
        "from github import Github\n",
        "import arxiv\n",
        "\n",
        "import time\n",
        "import google.generativeai as genai\n",
        "#from google.generativeai.types import RateLimitError #Import RateLimitError\n",
        "from google.api_core.exceptions import ResourceExhausted\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Credentials\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "github_token = userdata.get('GITHUB_TOKEN')\n",
        "email_password = userdata.get('EMAIL_PASSWORD')\n",
        "email_id=userdata.get('EMAIL_ID')\n",
        "\n",
        "# Initialize clients\n",
        "github = Github(github_token)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Gemini API\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "generation_config = genai.types.GenerationConfig(\n",
        "    temperature=0.5,\n",
        "    top_p=1,\n",
        "    top_k=1,\n",
        "    max_output_tokens=2048,\n",
        ")\n",
        "safety_settings = [\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "    },\n",
        "]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kHrpc_56-Qem"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check which models are available\n",
        "print('Available models:')\n",
        "models=genai.list_models()\n",
        "for model in models:\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "v2OEG8SFB0ob",
        "outputId": "343e4b39-6dbe-4b92-c996-0afba21796e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available models:\n",
            "models/chat-bison-001\n",
            "models/text-bison-001\n",
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-1.5-pro-experimental\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate the model - gemini-2.0-flash\n",
        "model = genai.GenerativeModel(model_name=\"models/gemini-2.0-flash\",\n",
        "                             generation_config=generation_config,\n",
        "                            safety_settings=safety_settings)"
      ],
      "metadata": {
        "id": "I6y-rfcUSqHR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1sYq5A59B3K"
      },
      "source": [
        "## Core Functions\n",
        "### 1. Content Aggregation\n",
        "Fetch research papers and GitHub repos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KvE4RZ7X9B3L"
      },
      "outputs": [],
      "source": [
        "def fetch_ai_content():\n",
        "    \"\"\"Fetch latest GenAI research and projects\"\"\"\n",
        "\n",
        "    # Get arXiv papers\n",
        "    arxiv_client = arxiv.Client()\n",
        "    search = arxiv.Search(\n",
        "        query=\"generative AI\",\n",
        "        max_results=5,\n",
        "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
        "    )\n",
        "    papers = [result for result in arxiv_client.results(search)]\n",
        "\n",
        "    # Get GitHub repos\n",
        "    repos = github.search_repositories(\n",
        "        query=\"generative AI created:>2025-01-01\",\n",
        "        sort=\"updated\"\n",
        "    )[:3]\n",
        "\n",
        "    return {\n",
        "        \"papers\": papers,\n",
        "        \"repos\": repos\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wM8jKvF9B3L"
      },
      "source": [
        "### 2. Analysis\n",
        "Generate technical summaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FQc1Pq6V9B3L"
      },
      "outputs": [],
      "source": [
        "def analyze_content(content):\n",
        "    \"\"\"Process content with GPT-4\"\"\"\n",
        "\n",
        "    # Generate paper summaries\n",
        "    paper_summaries = []\n",
        "    for paper in content['papers']:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"Summarize this AI paper in 3 bullet points:\\n{paper.summary}\"\n",
        "            }],\n",
        "            temperature=0.3\n",
        "        )\n",
        "        paper_summaries.append(response.choices[0].message.content)\n",
        "\n",
        "    # Analyze GitHub repos\n",
        "    repo_analyses = []\n",
        "    for repo in content['repos']:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"Explain the technical significance of {repo.name}:\\n{repo.description}\"\n",
        "            }],\n",
        "            temperature=0.5\n",
        "        )\n",
        "        repo_analyses.append(response.choices[0].message.content)\n",
        "\n",
        "    return {\n",
        "        \"papers\": paper_summaries,\n",
        "        \"repos\": repo_analyses\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_content(content):\n",
        "    \"\"\"Process content with Gemini Pro\"\"\"\n",
        "\n",
        "    # Generate paper summaries\n",
        "    paper_summaries = []\n",
        "    for paper in content['papers']:\n",
        "        def generate_gemini_text_with_retry(prompt):\n",
        "            retries = 3\n",
        "            wait_time = 1\n",
        "            for attempt in range(retries):\n",
        "                try:\n",
        "                    response = model.generate_content(prompt)\n",
        "                    return response.text\n",
        "                except ResourceExhausted as e:\n",
        "                    if attempt == retries - 1:\n",
        "                        raise e\n",
        "                    else:\n",
        "                        time.sleep(wait_time)\n",
        "                        wait_time *= 2\n",
        "\n",
        "        prompt = f\"Summarize this AI paper in 3 bullet points:\\n{paper.summary}\"\n",
        "        summary = generate_gemini_text_with_retry(prompt)\n",
        "        paper_summaries.append(summary)\n",
        "\n",
        "\n",
        "    # Analyze GitHub repos\n",
        "    repo_analyses = []\n",
        "    for repo in content['repos']:\n",
        "        def generate_gemini_text_with_retry(prompt):\n",
        "            retries = 3\n",
        "            wait_time = 1\n",
        "            for attempt in range(retries):\n",
        "                try:\n",
        "                    response = model.generate_content(prompt)\n",
        "                    return response.text\n",
        "                except ResourceExhausted as e:\n",
        "                   if attempt == retries - 1:\n",
        "                       raise e\n",
        "                   else:\n",
        "                       time.sleep(wait_time)\n",
        "                       wait_time *= 2\n",
        "        prompt = f\"Explain the technical significance of {repo.name}:\\n{repo.description}\"\n",
        "        analysis = generate_gemini_text_with_retry(prompt)\n",
        "        repo_analyses.append(analysis)\n",
        "\n",
        "    return {\n",
        "        \"papers\": paper_summaries,\n",
        "        \"repos\": repo_analyses\n",
        "    }"
      ],
      "metadata": {
        "id": "tUgHyXcU_hzZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YvJ0W1k9B3L"
      },
      "source": [
        "### 3. Newsletter Generation\n",
        "Create formatted email content:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_newsletter(analysis):\n",
        "    \"\"\"Create HTML newsletter with Gemini Models\"\"\"\n",
        "\n",
        "    # Generate header\n",
        "    header = \"# ðŸš€ Weekly GenAI Digest\\n\\n\"\n",
        "\n",
        "    # Top papers section\n",
        "    papers_section = \"## ðŸ“„ Top Research Papers\\n\"\n",
        "    papers_section += \"\\n\".join([f\"- {summary}\" for summary in analysis['papers']])\n",
        "\n",
        "    # GitHub projects section\n",
        "    repos_section = \"\\n\\n## ðŸ’» Trending Code Repos\\n\"\n",
        "    repos_section += \"\\n\".join([f\"- {analysis}\" for analysis in analysis['repos']])\n",
        "\n",
        "    # Final assembly\n",
        "    full_content = header + papers_section + repos_section\n",
        "\n",
        "\n",
        "    # Format with Gemini\n",
        "    def generate_gemini_text_with_retry(prompt):\n",
        "        retries = 3\n",
        "        wait_time = 1\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                response = model.generate_content(prompt)\n",
        "                return response.text\n",
        "            except ResourceExhausted as e:\n",
        "                if attempt == retries - 1:\n",
        "                    raise e\n",
        "                else:\n",
        "                    time.sleep(wait_time)\n",
        "                    wait_time *= 2\n",
        "\n",
        "    prompt = f\"\"\"You are an expert in HTML and newsletter formatting. Your task is to convert the following plain text newsletter content into well-structured, readable HTML. The output should be a complete HTML document with inline CSS styles for basic formatting. Make sure the format is suitable for sending as an HTML email.\n",
        "\n",
        "Here are the formatting guidelines:\n",
        "\n",
        "*   **Headings:**\n",
        "    *   Top-level headings (starting with `#`) should be wrapped in `<h1>` tags.\n",
        "    *   Second-level headings (starting with `##`) should be wrapped in `<h2>` tags.\n",
        "\n",
        "*   **Lists:**\n",
        "    *   Bullet points (lines starting with `-`) should be formatted as unordered lists (`<ul>`). Each bullet point should be inside a list item (`<li>`) tag.\n",
        "    *  If a bullet point contains a second bullet point (a line starting with `*`), the second list should be nested in the parent list item.\n",
        "\n",
        "*   **Inline Styling:**\n",
        "    *   Use inline CSS styles for basic formatting, such as `font-family`, `line-height`, `margin`, `color`, and `padding`.\n",
        "*   **General Structure:**\n",
        "    *   Ensure that the output is a valid HTML document that includes `<html>`, `<head>`, `<style>`, and `<body>` tags.\n",
        "\n",
        "*   **Other content:**\n",
        "    *   If there is other content that isn't a heading or list, wrap it in `<p>` tags.\n",
        "* Make sure that the content can be sent as html email in gmail\n",
        "\n",
        "Here is the newsletter content to format:\\n{full_content}\"\"\"\n",
        "    formatted_content = generate_gemini_text_with_retry(prompt)\n",
        "\n",
        "\n",
        "    return formatted_content"
      ],
      "metadata": {
        "id": "3rAsmMKG_tzc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7oO2-0K9B3M"
      },
      "source": [
        "## Execution Pipeline\n",
        "Run the complete workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx5vPw3_9B3M"
      },
      "outputs": [],
      "source": [
        "# Generate This Week's Issue\n",
        "content = fetch_ai_content()\n",
        "analysis = analyze_content(content)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsletter = generate_newsletter(analysis)\n",
        "\n",
        "print(\"Newsletter Generated!\\n\")\n",
        "print(newsletter)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AeB-jn_WYz7f",
        "outputId": "28f6ba46-e955-4a62-eb5c-4cd94ad95031"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Newsletter Generated!\n",
            "\n",
            "```html\n",
            "<!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "    <meta charset=\"UTF-8\">\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
            "    <title>Weekly GenAI Digest</title>\n",
            "    <style>\n",
            "        body {\n",
            "            font-family: Arial, sans-serif;\n",
            "            line-height: 1.6;\n",
            "            margin: 20px;\n",
            "            color: #333;\n",
            "        }\n",
            "        h1 {\n",
            "            font-size: 2em;\n",
            "            margin-bottom: 0.5em;\n",
            "            color: #0056b3;\n",
            "        }\n",
            "        h2 {\n",
            "            font-size: 1.5em;\n",
            "            margin-top: 1em;\n",
            "            margin-bottom: 0.5em;\n",
            "            color: #0056b3;\n",
            "        }\n",
            "        ul {\n",
            "            margin-bottom: 1em;\n",
            "            padding-left: 20px;\n",
            "        }\n",
            "        li {\n",
            "            margin-bottom: 0.5em;\n",
            "        }\n",
            "        p {\n",
            "            margin-bottom: 1em;\n",
            "        }\n",
            "    </style>\n",
            "</head>\n",
            "<body>\n",
            "    <h1 style=\"font-size: 2em; margin-bottom: 0.5em; color: #0056b3;\">ðŸš€ Weekly GenAI Digest</h1>\n",
            "\n",
            "    <h2 style=\"font-size: 1.5em; margin-top: 1em; margin-bottom: 0.5em; color: #0056b3;\">ðŸ“„ Top Research Papers</h2>\n",
            "    <ul>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>Language Models (LMs) are promising for accelerating scientific discovery, but current benchmarks primarily focus on solution generation, neglecting the crucial ability to falsify incorrect hypotheses.</strong>\n",
            "        </li>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>The paper introduces REFUTE, a new benchmark for evaluating LMs' ability to generate counterexamples to subtly incorrect solutions in algorithmic problem solving, using code execution for automatic evaluation.</strong>\n",
            "        </li>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>Current state-of-the-art LMs, even with code execution feedback, struggle to generate counterexamples for incorrect solutions in REFUTE, highlighting a significant gap in their reasoning capabilities and the need for benchmarks that promote falsification skills.</strong>\n",
            "        </li>\n",
            "    </ul>\n",
            "\n",
            "    <p style=\"margin-bottom: 1em;\">Here's a 3-bullet point summary of the AI paper:</p>\n",
            "    <ul>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>Problem:</strong> LLM-generated explanations for AI assistant recommendations are too verbose for ultra-small devices like smartwatches, hindering user understanding and action.\n",
            "        </li>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>Solution Explored:</strong> The paper investigated two approaches: (1) structuring LLM explanations using defined contextual components during prompting for spatial organization and (2) presenting explanations adaptively based on confidence levels for temporal adaptation.\n",
            "        </li>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>Findings & Implications:</strong> Structured explanations improved efficiency (reduced time to action, cognitive load) and acceptance, but decreased satisfaction due to lack of detail. Adaptive presentation was less effective than always-on structured explanations. The study provides design implications for personalizing the content and timing of LLM explanations on ultra-small devices.\n",
            "        </li>\n",
            "    </ul>\n",
            "\n",
            "    <p style=\"margin-bottom: 1em;\">Here's a 3-bullet point summary of the AI paper:</p>\n",
            "    <ul>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>ImageChain addresses the limitation of MLLMs in reasoning over image sequences by framing visual data as a multi-turn conversation, interleaving images with descriptive text to explicitly capture temporal dependencies.</strong> This approach moves beyond treating images independently.\n",
            "        </li>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>The framework is optimized for next-scene description, where the model predicts the description of a subsequent image based on preceding visual and textual context.</strong> This task encourages the model to understand the sequential nature of the image data.\n",
            "        </li>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>ImageChain demonstrates significant performance improvements in next-scene description (up to 19% in SimRate) and exhibits robust zero-shot out-of-domain performance in various applications, proving the effectiveness of instruction-tuning with a multi-turn conversation design for temporal reasoning.</strong> This shows the generalizability of the approach.\n",
            "        </li>\n",
            "    </ul>\n",
            "\n",
            "    <p style=\"margin-bottom: 1em;\">Here's a summary of the AI paper in three bullet points:</p>\n",
            "    <ul>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>The paper establishes a connection between the q-refined theta function of a log Calabi-Yau surface and the open mirror map, interpreting its coefficients as all-genus logarithmic two-point invariants.</strong> This extends previous work relating these concepts.\n",
            "        </li>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>It identifies and explains a discrepancy at higher genus between these logarithmic invariants and open Gromov-Witten invariants, expressing this difference in terms of relative invariants of an elliptic curve.</strong> This highlights a refinement beyond the initial connection.\n",
            "        </li>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>The paper demonstrates a correspondence between open invariants of a local Calabi-Yau and closed invariants of a related blown-up Calabi-Yau, generalizing existing results and equating winding-1 open-BPS invariants with closed Gopakumar-Vafa invariants.</strong> This provides a broader framework for understanding these invariants.\n",
            "        </li>\n",
            "    </ul>\n",
            "\n",
            "    <p style=\"margin-bottom: 1em;\">Here's a summary of the AI paper in three bullet points:</p>\n",
            "    <ul>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>Leveraging Redundancy:</strong> The paper explores Generalized-Bicycle (GB) quantum error-correcting codes, which possess naturally redundant minimum-weight stabilizer generators, and demonstrates that utilizing this redundancy significantly improves decoding accuracy.\n",
            "        </li>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>Short Codes with High Performance:</strong> The authors designed short GB codes with favorable properties (large dimensions, distances, and syndrome distances) and efficient syndrome measurement schedules, allowing for near-time-optimal fault-tolerant error correction.\n",
            "        </li>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>Sliding Window Decoding:</strong> They implemented a sliding window sequential decoding protocol and showed that even a small window size (T=2) achieves near-optimal logical error rates compared to larger window sizes, with improved performance compared to codes without redundant stabilizer generators.\n",
            "        </li>\n",
            "    </ul>\n",
            "\n",
            "    <h2 style=\"font-size: 1.5em; margin-top: 1em; margin-bottom: 0.5em; color: #0056b3;\">ðŸ’» Trending Code Repos</h2>\n",
            "    <ul>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            The tags you provided, \"Adobe-Photoshop-Ai-generative-2024-for-free,\" \"Adobe Photoshop 2024 free,\" \"Download Adobe Photoshop 2024 for free,\" and \"Download free Adobe Photoshop 2024,\"  are highly suggestive of <strong>piracy and potential malware distribution.</strong>\n",
            "        </li>\n",
            "    </ul>\n",
            "    <p style=\"margin-bottom: 1em;\">Let's break down the technical significance (and dangers) of this in the context of Adobe Photoshop 2024 and AI:</p>\n",
            "    <p style=\"margin-bottom: 1em;\"><strong>1. Adobe Photoshop 2024 (Legitimate):</strong></p>\n",
            "    <ul>\n",
            "        <li style=\"margin-bottom: 0.5em;\">\n",
            "            <strong>AI Integration:</strong>  The <em>real</em> technical significance of Photoshop 2024 (and recent versions) lies in its deep integration of AI, particularly Generative AI.  This includes features like:\n",
            "            <ul>\n",
            "                <li style=\"margin-bottom: 0.5em;\">\n",
            "                    <strong>Generative Fill:</strong>  Uses AI models to intelligently fill in areas of an image based on text prompts or surrounding content.  This dramatically speeds up complex editing tasks like removing objects, extending backgrounds, or adding new elements.\n",
            "                </li>\n",
            "                <li style=\"margin-bottom: 0.5em;\">\n",
            "                    <strong>Generative Expand:</strong>  Expands the canvas of an image and uses AI to generate realistic content to fill the newly created space.\n",
            "                </li>\n",
            "                <li style=\"margin-bottom: 0.5em;\">\n",
            "                    <strong>Object Selection:</strong>  AI-powered tools that can automatically and accurately select objects in an image, making masking and manipulation much easier.\n",
            "                </li>\n",
            "                <li style=\"margin-bottom: 0.5em\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsletter=newsletter.strip('```html\\n').strip('\\n```')"
      ],
      "metadata": {
        "id": "PyXFdKeparIR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsletter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "3hc0_Neta7z5",
        "outputId": "66f83a62-ce66-45f2-d6dc-6896b217ca71"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n    <meta charset=\"UTF-8\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n    <title>Weekly GenAI Digest</title>\\n    <style>\\n        body {\\n            font-family: Arial, sans-serif;\\n            line-height: 1.6;\\n            margin: 20px;\\n            color: #333;\\n        }\\n        h1 {\\n            font-size: 2em;\\n            margin-bottom: 0.5em;\\n            color: #0056b3;\\n        }\\n        h2 {\\n            font-size: 1.5em;\\n            margin-top: 1em;\\n            margin-bottom: 0.5em;\\n            color: #0056b3;\\n        }\\n        ul {\\n            margin-bottom: 1em;\\n            padding-left: 20px;\\n        }\\n        li {\\n            margin-bottom: 0.5em;\\n        }\\n        p {\\n            margin-bottom: 1em;\\n        }\\n    </style>\\n</head>\\n<body>\\n    <h1 style=\"font-size: 2em; margin-bottom: 0.5em; color: #0056b3;\">ðŸš€ Weekly GenAI Digest</h1>\\n\\n    <h2 style=\"font-size: 1.5em; margin-top: 1em; margin-bottom: 0.5em; color: #0056b3;\">ðŸ“„ Top Research Papers</h2>\\n    <ul>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>Language Models (LMs) are promising for accelerating scientific discovery, but current benchmarks primarily focus on solution generation, neglecting the crucial ability to falsify incorrect hypotheses.</strong>\\n        </li>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>The paper introduces REFUTE, a new benchmark for evaluating LMs\\' ability to generate counterexamples to subtly incorrect solutions in algorithmic problem solving, using code execution for automatic evaluation.</strong>\\n        </li>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>Current state-of-the-art LMs, even with code execution feedback, struggle to generate counterexamples for incorrect solutions in REFUTE, highlighting a significant gap in their reasoning capabilities and the need for benchmarks that promote falsification skills.</strong>\\n        </li>\\n    </ul>\\n\\n    <p style=\"margin-bottom: 1em;\">Here\\'s a 3-bullet point summary of the AI paper:</p>\\n    <ul>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>Problem:</strong> LLM-generated explanations for AI assistant recommendations are too verbose for ultra-small devices like smartwatches, hindering user understanding and action.\\n        </li>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>Solution Explored:</strong> The paper investigated two approaches: (1) structuring LLM explanations using defined contextual components during prompting for spatial organization and (2) presenting explanations adaptively based on confidence levels for temporal adaptation.\\n        </li>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>Findings & Implications:</strong> Structured explanations improved efficiency (reduced time to action, cognitive load) and acceptance, but decreased satisfaction due to lack of detail. Adaptive presentation was less effective than always-on structured explanations. The study provides design implications for personalizing the content and timing of LLM explanations on ultra-small devices.\\n        </li>\\n    </ul>\\n\\n    <p style=\"margin-bottom: 1em;\">Here\\'s a 3-bullet point summary of the AI paper:</p>\\n    <ul>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>ImageChain addresses the limitation of MLLMs in reasoning over image sequences by framing visual data as a multi-turn conversation, interleaving images with descriptive text to explicitly capture temporal dependencies.</strong> This approach moves beyond treating images independently.\\n        </li>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>The framework is optimized for next-scene description, where the model predicts the description of a subsequent image based on preceding visual and textual context.</strong> This task encourages the model to understand the sequential nature of the image data.\\n        </li>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>ImageChain demonstrates significant performance improvements in next-scene description (up to 19% in SimRate) and exhibits robust zero-shot out-of-domain performance in various applications, proving the effectiveness of instruction-tuning with a multi-turn conversation design for temporal reasoning.</strong> This shows the generalizability of the approach.\\n        </li>\\n    </ul>\\n\\n    <p style=\"margin-bottom: 1em;\">Here\\'s a summary of the AI paper in three bullet points:</p>\\n    <ul>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>The paper establishes a connection between the q-refined theta function of a log Calabi-Yau surface and the open mirror map, interpreting its coefficients as all-genus logarithmic two-point invariants.</strong> This extends previous work relating these concepts.\\n        </li>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>It identifies and explains a discrepancy at higher genus between these logarithmic invariants and open Gromov-Witten invariants, expressing this difference in terms of relative invariants of an elliptic curve.</strong> This highlights a refinement beyond the initial connection.\\n        </li>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>The paper demonstrates a correspondence between open invariants of a local Calabi-Yau and closed invariants of a related blown-up Calabi-Yau, generalizing existing results and equating winding-1 open-BPS invariants with closed Gopakumar-Vafa invariants.</strong> This provides a broader framework for understanding these invariants.\\n        </li>\\n    </ul>\\n\\n    <p style=\"margin-bottom: 1em;\">Here\\'s a summary of the AI paper in three bullet points:</p>\\n    <ul>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>Leveraging Redundancy:</strong> The paper explores Generalized-Bicycle (GB) quantum error-correcting codes, which possess naturally redundant minimum-weight stabilizer generators, and demonstrates that utilizing this redundancy significantly improves decoding accuracy.\\n        </li>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>Short Codes with High Performance:</strong> The authors designed short GB codes with favorable properties (large dimensions, distances, and syndrome distances) and efficient syndrome measurement schedules, allowing for near-time-optimal fault-tolerant error correction.\\n        </li>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>Sliding Window Decoding:</strong> They implemented a sliding window sequential decoding protocol and showed that even a small window size (T=2) achieves near-optimal logical error rates compared to larger window sizes, with improved performance compared to codes without redundant stabilizer generators.\\n        </li>\\n    </ul>\\n\\n    <h2 style=\"font-size: 1.5em; margin-top: 1em; margin-bottom: 0.5em; color: #0056b3;\">ðŸ’» Trending Code Repos</h2>\\n    <ul>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            The tags you provided, \"Adobe-Photoshop-Ai-generative-2024-for-free,\" \"Adobe Photoshop 2024 free,\" \"Download Adobe Photoshop 2024 for free,\" and \"Download free Adobe Photoshop 2024,\"  are highly suggestive of <strong>piracy and potential malware distribution.</strong>\\n        </li>\\n    </ul>\\n    <p style=\"margin-bottom: 1em;\">Let\\'s break down the technical significance (and dangers) of this in the context of Adobe Photoshop 2024 and AI:</p>\\n    <p style=\"margin-bottom: 1em;\"><strong>1. Adobe Photoshop 2024 (Legitimate):</strong></p>\\n    <ul>\\n        <li style=\"margin-bottom: 0.5em;\">\\n            <strong>AI Integration:</strong>  The <em>real</em> technical significance of Photoshop 2024 (and recent versions) lies in its deep integration of AI, particularly Generative AI.  This includes features like:\\n            <ul>\\n                <li style=\"margin-bottom: 0.5em;\">\\n                    <strong>Generative Fill:</strong>  Uses AI models to intelligently fill in areas of an image based on text prompts or surrounding content.  This dramatically speeds up complex editing tasks like removing objects, extending backgrounds, or adding new elements.\\n                </li>\\n                <li style=\"margin-bottom: 0.5em;\">\\n                    <strong>Generative Expand:</strong>  Expands the canvas of an image and uses AI to generate realistic content to fill the newly created space.\\n                </li>\\n                <li style=\"margin-bottom: 0.5em;\">\\n                    <strong>Object Selection:</strong>  AI-powered tools that can automatically and accurately select objects in an image, making masking and manipulation much easier.\\n                </li>\\n                <li style=\"margin-bottom: 0.5e'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7vj1RSo9B3M"
      },
      "source": [
        "## Email Distribution\n",
        "Send newsletter to subscribers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "VdOQxq7V9B3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7faec6ae-ade1-4de3-e766-82299b1b39bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Newsletter sent successfully!\n"
          ]
        }
      ],
      "source": [
        "import smtplib\n",
        "import ssl\n",
        "from email.mime.text import MIMEText\n",
        "\n",
        "def send_newsletter(recipients):\n",
        "    \"\"\"Distribute via Gmail\"\"\"\n",
        "\n",
        "    msg = MIMEText(newsletter, 'html')\n",
        "    msg['Subject'] = 'Your Weekly GenAI Update'\n",
        "    msg['From'] = 'sender@gmail.com'\n",
        "    msg['To'] = ', '.join(recipients)\n",
        "    msg.replace_header('Content-Type', 'text/html; charset=\"utf-8\"')\n",
        "\n",
        "\n",
        "    context = ssl.create_default_context()\n",
        "    with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context) as server:\n",
        "        server.login(email_id, email_password)\n",
        "        server.sendmail(\n",
        "            msg['From'], recipients, msg.as_string()\n",
        "        )\n",
        "\n",
        "# Send Email\n",
        "test_recipients = [\"youremail@gmail.com\"]  # Replace with correct email\n",
        "send_newsletter(test_recipients)\n",
        "print(\"Newsletter sent successfully!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}